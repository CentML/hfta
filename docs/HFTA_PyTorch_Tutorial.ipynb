{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HFTA Tutorial.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItXfxkxvosLH"
      },
      "source": [
        "# HFTA Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg62Pmz3o83v"
      },
      "source": [
        "This notebook demonstrates the way to integrate HFTA to a simple mnist training example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4DN769E2O_R"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHFB4zBKYp4k"
      },
      "source": [
        "Install the HFTA library from GitHub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAT7bgRwYJ8e"
      },
      "source": [
        "!pip install git+https://github.com/UofT-EcoSystem/hfta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Kh8dzdr5gI"
      },
      "source": [
        "### Demo with a benchmark\n",
        "\n",
        "Here is a demo run on one of the benchmarks provided in the `hfta` GitHub repo.\n",
        "\n",
        "Check [here](https://github.com/UofT-EcoSystem/hfta/tree/main/examples/mobilenet) for the code of this example (MobileNet V2).\n",
        "\n",
        "If you want to see a simpler example on how to utilize HFTA on a normal pytorch model, check the next section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L07tB-u7YXVE"
      },
      "source": [
        "# We need to sync down the GitHub repo to run the benchmarks\n",
        "!git clone https://github.com/UofT-EcoSystem/hfta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGpUu0yIYeFo"
      },
      "source": [
        "# Run the MobileNet V2 benchmark\n",
        "!python hfta/examples/mobilenet/main.py --version v2 --epochs 5 --amp --eval --dataset cifar10 --device cuda --lr 0.01 0.02 0.03 --hfta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6fX_L6cY55P"
      },
      "source": [
        "## Train a mnist model without HFTA\n",
        "\n",
        "In this section, we provide a simpler example to show how to modify a normal pytorch model to its HFTA version.\n",
        "\n",
        "We train a simple neural network with two convolutional layers and two fully connected layers, together with some max_pool and dropout layers. This model is used to train a mnist dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9ZuRE7hZ5K_"
      },
      "source": [
        "### Define the non-HFTA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIipDP9YqC2v"
      },
      "source": [
        "import time\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "try:\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xm\n",
        "  import torch_xla.debug.metrics as met\n",
        "except ImportError:\n",
        "  pass\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "    self.max_pool2d = nn.MaxPool2d(2)\n",
        "    self.fc1 = nn.Linear(9216, 128)\n",
        "    self.fc2 = nn.Linear(128, 10)\n",
        "    self.dropout1 = nn.Dropout2d(0.25)\n",
        "    self.dropout2 = nn.Dropout2d(0.5)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.max_pool2d(x)\n",
        "    x = self.dropout1(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.fc2(x)\n",
        "    output = F.log_softmax(x, dim=1)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIlg5eVScoWS"
      },
      "source": [
        "### Define the training and testing loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiJkw55Hcwp6"
      },
      "source": [
        "def train(config, model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    if config[\"device\"] == 'xla':\n",
        "      xm.optimizer_step(optimizer, barrier=True)\n",
        "    else:\n",
        "      optimizer.step()\n",
        "    if batch_idx % config[\"log_interval\"] == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch,\n",
        "          batch_idx * len(data),\n",
        "          len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader),\n",
        "          loss.item(),\n",
        "      ))\n",
        "      if config[\"dry_run\"]:\n",
        "        break\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      N = target.size(0)\n",
        "      output = model(data)\n",
        "      test_loss += F.nll_loss(output, target,\n",
        "                              reduction='none').view(-1, N).sum(dim=1)\n",
        "      pred = output.argmax(dim=1, keepdim=True)\n",
        "      correct += pred.eq(target.view_as(pred)).view(-1, N).sum(dim=1)\n",
        "\n",
        "  length = len(test_loader.dataset)\n",
        "  test_loss /= length\n",
        "  loss_str = [\"%.4f\" % e for e in test_loss]\n",
        "  correct_str = [\n",
        "      \"%d/%d(%.2lf%%)\" % (e, length, 100. * e / length) for e in correct\n",
        "  ]\n",
        "  print('Test set: \\tAverage loss: {}, \\n \\t\\t\\tAccuracy: {}\\n'.format(\n",
        "      loss_str, correct_str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43Nr19KJc4MT"
      },
      "source": [
        "### Define the main loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhts2SMCc7B4"
      },
      "source": [
        "def main(config):\n",
        "  random.seed(1)\n",
        "  np.random.seed(1)\n",
        "  torch.manual_seed(1)\n",
        "\n",
        "  device = (torch.device(config[\"device\"])\n",
        "            if config[\"device\"] in {'cpu', 'cuda'} else xm.xla_device())\n",
        "\n",
        "  kwargs = {'batch_size': config[\"batch_size\"]}\n",
        "  kwargs.update({'num_workers': 1, 'pin_memory': True, 'shuffle': True},)\n",
        "\n",
        "  transform = transforms.Compose(\n",
        "      [transforms.ToTensor(),\n",
        "       transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "  dataset1 = datasets.MNIST('./data',\n",
        "                            train=True,\n",
        "                            download=True,\n",
        "                            transform=transform)\n",
        "  dataset2 = datasets.MNIST('./data', train=False, transform=transform)\n",
        "  train_loader = torch.utils.data.DataLoader(dataset1, **kwargs)\n",
        "  test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)\n",
        "\n",
        "  model = Net().to(device)\n",
        "\n",
        "  optimizer = optim.Adadelta(\n",
        "      model.parameters(),\n",
        "      lr=config[\"lr\"][0],\n",
        "  )\n",
        "\n",
        "  start = time.perf_counter()\n",
        "  for epoch in range(1, config[\"epochs\"] + 1):\n",
        "    now = time.perf_counter()\n",
        "    train(config, model, device, train_loader, optimizer, epoch)\n",
        "    print('Epoch {} took {} s!'.format(epoch, time.perf_counter() - now))\n",
        "  end = time.perf_counter()\n",
        "\n",
        "  test(model, device, test_loader)\n",
        "\n",
        "  print('All jobs Finished, Each epoch took {} s on average!'.format(\n",
        "      (end - start) / config[\"epochs\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35jv_fzP-llU"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VwstttHmxZF"
      },
      "source": [
        "config = {\n",
        "    \"device\": \"cuda\",\n",
        "    \"batch_size\": 64,\n",
        "    \"lr\": [1.0],\n",
        "    \"gamma\": 0.7,\n",
        "    \"epochs\": 4,\n",
        "    \"seed\": 1,\n",
        "    \"log_interval\": 500,\n",
        "    \"dry_run\": False,\n",
        "    \"save_model\": False,\n",
        "}\n",
        "\n",
        "print(config)\n",
        "main(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzr_a67BZyJc"
      },
      "source": [
        "## Improve hardware utilization with HFTA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcehAMUiaE3Q"
      },
      "source": [
        "### How to modify a mnist model to use HFTA?\n",
        "\n",
        "Up until now, you need to make modifications to your model, training loop, and data shape to utilize the benefits from HFTA. The HFTA package provides convenient converter to facilitate the process and we are working on automating this process now.\n",
        "\n",
        "Please check the comments in the code to understand what need to be done. In this example, we fuse multiple models with different learning rates together with HFTA to improve the hardware utilizations. Beyond this example, in addition to the learning rate, there are more hyperparameters can be fused."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqz5n9L3qLcw"
      },
      "source": [
        "#### Modify the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQGHQtpReYWA"
      },
      "source": [
        "from __future__ import print_function\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "try:\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xm\n",
        "  import torch_xla.debug.metrics as met\n",
        "except ImportError:\n",
        "  pass\n",
        "\n",
        "# Use helper functions from hfta package to convert your operators and optimizors\n",
        "from hfta.ops import get_hfta_op_for\n",
        "from hfta.optim import get_hfta_optim_for\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "  # When initializing the model, save the number of fused models (B),\n",
        "  # and convert the default operators to HFTA version with get_hfta_op_for(<default>, B).\n",
        "  def __init__(self, B=0):\n",
        "    super(Net, self).__init__()\n",
        "    self.B = B\n",
        "    self.conv1 = get_hfta_op_for(nn.Conv2d, B=B)(1, 32, 3, 1)\n",
        "    self.conv2 = get_hfta_op_for(nn.Conv2d, B=B)(32, 64, 3, 1)\n",
        "    self.max_pool2d = get_hfta_op_for(nn.MaxPool2d, B=B)(2)\n",
        "    self.fc1 = get_hfta_op_for(nn.Linear, B=B)(9216, 128)\n",
        "    self.fc2 = get_hfta_op_for(nn.Linear, B=B)(128, 10)\n",
        "    self.dropout1 = get_hfta_op_for(nn.Dropout2d, B=B)(0.25)\n",
        "    self.dropout2 = get_hfta_op_for(nn.Dropout2d, B=B)(0.5)\n",
        "\n",
        "  # Minor modifications to the forward pass on special operators.\n",
        "  # Check the documentation of each operator for details.\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.max_pool2d(x)\n",
        "    x = self.dropout1(x)\n",
        "\n",
        "    if self.B > 0:\n",
        "      x = torch.flatten(x, 2)\n",
        "      x = x.transpose(0, 1)\n",
        "    else:\n",
        "      x = torch.flatten(x, 1)\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    if self.B > 0:\n",
        "      output = F.log_softmax(x, dim=2)\n",
        "    else:\n",
        "      output = F.log_softmax(x, dim=1)\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6Bn9_jCqQTv"
      },
      "source": [
        "#### Modify the training and testing loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjokanLrqTyp"
      },
      "source": [
        "def train(config, model, device, train_loader, optimizer, epoch, B):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Need to combine multiple batches of input data to feed into the fused model\n",
        "    if B > 0:\n",
        "      N = target.size(0)\n",
        "      data = data.unsqueeze(1).expand(-1, B, -1, -1, -1)\n",
        "      target = target.repeat(B)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "\n",
        "    # Also need to modify the loss function to take consider on fused models\n",
        "    if B > 0:\n",
        "      loss = B * F.nll_loss(output.view(B * N, -1), target)\n",
        "    else:\n",
        "      loss = F.nll_loss(output, target)\n",
        "\n",
        "    loss.backward()\n",
        "    if config[\"device\"] == 'xla':\n",
        "      xm.optimizer_step(optimizer, barrier=True)\n",
        "    else:\n",
        "      optimizer.step()\n",
        "    if batch_idx % config[\"log_interval\"] == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "      if config[\"dry_run\"]:\n",
        "        break\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, B):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      N = target.size(0)\n",
        "\n",
        "      # Need to combine multiple batches of input data to feed into the fused model\n",
        "      if B > 0:\n",
        "        data = data.unsqueeze(1).expand(-1, B, -1, -1, -1)\n",
        "        target = target.repeat(B)\n",
        "\n",
        "      output = model(data)\n",
        "\n",
        "      # Change the shape of the output for summing up the loss\n",
        "      if B > 0:\n",
        "        output = output.view(B * N, -1)\n",
        "\n",
        "      test_loss += F.nll_loss(output, target,\n",
        "                              reduction='none').view(-1, N).sum(dim=1)\n",
        "      pred = output.argmax(dim=1, keepdim=True)\n",
        "      correct += pred.eq(target.view_as(pred)).view(-1, N).sum(dim=1)\n",
        "\n",
        "  length = len(test_loader.dataset)\n",
        "  test_loss /= length\n",
        "  loss_str = [\"%.4f\" % e for e in test_loss]\n",
        "  correct_str = [\n",
        "      \"%d/%d(%.2lf%%)\" % (e, length, 100. * e / length) for e in correct\n",
        "  ]\n",
        "  print('Test set: \\tAverage loss: {}, \\n \\t\\t\\tAccuracy: {}\\n'.format(\n",
        "      loss_str, correct_str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0Fvlq5Sqc2v"
      },
      "source": [
        "#### Modify the main loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00ENhJCRqfAU"
      },
      "source": [
        "def main(config):\n",
        "  random.seed(config[\"seed\"])\n",
        "  np.random.seed(config[\"seed\"])\n",
        "  torch.manual_seed(config[\"seed\"])\n",
        "\n",
        "  device = (torch.device(config[\"device\"])\n",
        "            if config[\"device\"] in {'cpu', 'cuda'} else xm.xla_device())\n",
        "\n",
        "  kwargs = {'batch_size': config[\"batch_size\"]}\n",
        "  kwargs.update({'num_workers': 1, 'pin_memory': True, 'shuffle': True},)\n",
        "\n",
        "  transform = transforms.Compose(\n",
        "      [transforms.ToTensor(),\n",
        "       transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "  # Detect the number of fused models from the number of provided LR's\n",
        "  B = len(config[\"lr\"]) if config[\"use_hfta\"] else 0\n",
        "\n",
        "  dataset1 = datasets.MNIST('./data',\n",
        "                            train=True,\n",
        "                            download=True,\n",
        "                            transform=transform)\n",
        "  dataset2 = datasets.MNIST('./data', train=False, transform=transform)\n",
        "  train_loader = torch.utils.data.DataLoader(dataset1, **kwargs)\n",
        "  test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)\n",
        "\n",
        "  # Create the model and specify the number of fused models (B)\n",
        "  model = Net(B).to(device)\n",
        "\n",
        "  print('B={} lr={}'.format(B, config[\"lr\"]), file=sys.stderr)\n",
        "\n",
        "  # Convert the default optimizor (pytorch Adadelta) to HFTA version with get_hfta_optim_for(<default>, B).\n",
        "  optimizer = get_hfta_optim_for(optim.Adadelta, B=B)(\n",
        "      model.parameters(),\n",
        "      lr=config[\"lr\"] if B > 0 else config[\"lr\"][0],\n",
        "  )\n",
        "\n",
        "  start = time.perf_counter()\n",
        "  for epoch in range(1, config[\"epochs\"] + 1):\n",
        "    now = time.perf_counter()\n",
        "    train(config, model, device, train_loader, optimizer, epoch, B)\n",
        "    print('Epoch {} took {} s!'.format(epoch, time.perf_counter() - now))\n",
        "  end = time.perf_counter()\n",
        "\n",
        "  test(model, device, test_loader, B)\n",
        "\n",
        "  print('All jobs Finished, Each epoch took {} s on average!'.format(\n",
        "      (end - start) / (max(B, 1) * config[\"epochs\"])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EKBmbnYaM63"
      },
      "source": [
        "### Train a single HFTA-enhanced mnist model\n",
        "\n",
        "Note that this run may be slower than the non-HFTA version because enabling HFTA has a small amount of overhead. Here, we are only training a single model, so the overhead slows down the training. With more models being fused, the overhead is amortized resulting an overall improvement eventually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t6_2bCoabCN"
      },
      "source": [
        "# Enable HFTA, but not fusing models\n",
        "# Only 1 model is trained\n",
        "config = {\n",
        "    \"use_hfta\": True,\n",
        "    \"device\": \"cuda\",\n",
        "    \"batch_size\": 64,\n",
        "    \"lr\": [0.1],\n",
        "    \"gamma\": 0.7,\n",
        "    \"epochs\": 4,\n",
        "    \"seed\": 1,\n",
        "    \"log_interval\": 500,\n",
        "    \"dry_run\": False,\n",
        "    \"save_model\": False,\n",
        "}\n",
        "\n",
        "\n",
        "print(config)\n",
        "main(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6JFJ8laac_b"
      },
      "source": [
        "### Train fused HFTA-enhanced mnist model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N44NF4HoalOh"
      },
      "source": [
        "# Enable HFTA and fuse 6 models\n",
        "config = {\n",
        "    \"use_hfta\": True,\n",
        "    \"device\": \"cuda\",\n",
        "    \"batch_size\": 64,\n",
        "    \"lr\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
        "    \"gamma\": 0.7,\n",
        "    \"epochs\": 4,\n",
        "    \"seed\": 1,\n",
        "    \"log_interval\": 500,\n",
        "    \"dry_run\": False,\n",
        "    \"save_model\": False,\n",
        "}\n",
        "\n",
        "\n",
        "print(config)\n",
        "main(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFEmZ5zq-llk"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Based on the time each epoch takes when training the non-HFTA and HFTA version of the same model, we can see that HFTA helps to increase the throughput of the training, especially on a large hardware. Check our [paper](https://arxiv.org/pdf/2102.02344.pdf) for more details.\n",
        "\n",
        "We are working to make integrating HFTA to a normal pytorch model more convenient."
      ]
    }
  ]
}