{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HFTA Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "51675c8444e8435ca60180706ab7cd8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_35a868d5339e48b29ce6e643c3e75f15",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5d8a5c92f97e40468a0f8379c98bfaa3",
              "IPY_MODEL_c6bcd6e7256649929273f9061ff48f21"
            ]
          }
        },
        "35a868d5339e48b29ce6e643c3e75f15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5d8a5c92f97e40468a0f8379c98bfaa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_84634eff4a1342359cd0cfdeeb2b6107",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9912422,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9912422,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_77a998ae706448ba8d54c4ae5b55cd3d"
          }
        },
        "c6bcd6e7256649929273f9061ff48f21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1ede96c4d4ca479e85076ebc6dc80889",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9913344/? [00:00&lt;00:00, 59762068.08it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2423ed4ce4b54d56ab4bc501bd02e3ea"
          }
        },
        "84634eff4a1342359cd0cfdeeb2b6107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "77a998ae706448ba8d54c4ae5b55cd3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ede96c4d4ca479e85076ebc6dc80889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2423ed4ce4b54d56ab4bc501bd02e3ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "32f0a9145f404d8d9e58c76f1bcf7fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8fa0d5f8af9542238c744583af69f615",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_28c5a87170014d15b1547ccb8eb53750",
              "IPY_MODEL_8448978b77674b04ba52ea3bae1c0081"
            ]
          }
        },
        "8fa0d5f8af9542238c744583af69f615": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28c5a87170014d15b1547ccb8eb53750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d40617c96b194e4ba1f88c564eb14c0b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_526ccac9a4f04b5db6883109da8fd1bb"
          }
        },
        "8448978b77674b04ba52ea3bae1c0081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_271a71dc2370468c994f439635af4bc5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29696/? [00:00&lt;00:00, 185648.04it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a2b9c7ec480549b1879e49ee4bf17da6"
          }
        },
        "d40617c96b194e4ba1f88c564eb14c0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "526ccac9a4f04b5db6883109da8fd1bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "271a71dc2370468c994f439635af4bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a2b9c7ec480549b1879e49ee4bf17da6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28713bf12a84482fa48f34657309b970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3d15e9e7b5ce45e7b64ed44a7c512338",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_02b386588cfb425bb8b8d1da37ca23ae",
              "IPY_MODEL_6f82ca214b4a4f2a86d055895a2d3737"
            ]
          }
        },
        "3d15e9e7b5ce45e7b64ed44a7c512338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "02b386588cfb425bb8b8d1da37ca23ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_442f750c5d164eca9b876197d8c4d81b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1648877,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1648877,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_09efc251cf604e8886d3c2dc67e68a07"
          }
        },
        "6f82ca214b4a4f2a86d055895a2d3737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f5fa8090913f4ff5aef2eca6756d02e2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1649664/? [00:00&lt;00:00, 19799498.99it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_93cfebb4ebf74b0286e006f1c2c495f3"
          }
        },
        "442f750c5d164eca9b876197d8c4d81b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "09efc251cf604e8886d3c2dc67e68a07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f5fa8090913f4ff5aef2eca6756d02e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "93cfebb4ebf74b0286e006f1c2c495f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d7810aa95fc14a74831de82d54c8bbd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d67de91d55e041f1a7b5bd1a91a08793",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_af849cb35c9146948ad72a1962881394",
              "IPY_MODEL_7c608a5ce48d4602acf4567d4092caed"
            ]
          }
        },
        "d67de91d55e041f1a7b5bd1a91a08793": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af849cb35c9146948ad72a1962881394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1642a6479022482db6b5ed3c66da8864",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4542,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4542,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9e5522d0c9a4a518afbc26740348030"
          }
        },
        "7c608a5ce48d4602acf4567d4092caed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_78b27e47984241c0a3b6f6afd2b42d94",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5120/? [00:00&lt;00:00, 25038.23it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cd3f95d06f404c1eadf5f33cbe187023"
          }
        },
        "1642a6479022482db6b5ed3c66da8864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9e5522d0c9a4a518afbc26740348030": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "78b27e47984241c0a3b6f6afd2b42d94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cd3f95d06f404c1eadf5f33cbe187023": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItXfxkxvosLH"
      },
      "source": [
        "# A 10-minute Tutorial on How to Use `HFTA`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg62Pmz3o83v"
      },
      "source": [
        "This notebook demonstrates the way to integrate HFTA into a simple MNIST training example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4DN769E2O_R"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHFB4zBKYp4k"
      },
      "source": [
        "Install the HFTA library from GitHub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAT7bgRwYJ8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "508a7f99-4415-44e3-a30d-d40672c2d9ad"
      },
      "source": [
        "!pip install git+https://github.com/UofT-EcoSystem/hfta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/UofT-EcoSystem/hfta\n",
            "  Cloning https://github.com/UofT-EcoSystem/hfta to /tmp/pip-req-build-he46_x8d\n",
            "  Running command git clone -q https://github.com/UofT-EcoSystem/hfta /tmp/pip-req-build-he46_x8d\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from hfta==0.1.0) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hfta==0.1.0) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from hfta==0.1.0) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from hfta==0.1.0) (3.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from hfta==0.1.0) (5.4.8)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from hfta==0.1.0) (1.8.1+cu101)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.7/dist-packages (from hfta==0.1.0) (0.1.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->hfta==0.1.0) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->hfta==0.1.0) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->hfta==0.1.0) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->hfta==0.1.0) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->hfta==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->hfta==0.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt->hfta==0.1.0) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from hyperopt->hfta==0.1.0) (4.41.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt->hfta==0.1.0) (2.5.1)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt->hfta==0.1.0) (3.11.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt->hfta==0.1.0) (0.16.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt->hfta==0.1.0) (4.4.2)\n",
            "Building wheels for collected packages: hfta\n",
            "  Building wheel for hfta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hfta: filename=hfta-0.1.0-cp37-none-any.whl size=74269 sha256=e2c6643cb5b33224b748792a156fac92ca7f97f366b0017944e6e116fe73b7d3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8xzf8h_x/wheels/8d/02/01/48209526aba427578fbcd6b15919bac295ec9989de72f13ca6\n",
            "Successfully built hfta\n",
            "Installing collected packages: hfta\n",
            "Successfully installed hfta-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Kh8dzdr5gI"
      },
      "source": [
        "### Demo with a benchmark\n",
        "\n",
        "Here is a demo run on one of the benchmarks provided in the `hfta` GitHub repo to make sure HFTA is properly installed.\n",
        "\n",
        "Check [here](https://github.com/UofT-EcoSystem/hfta/tree/main/examples/mobilenet) for the code of this example (MobileNet V2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L07tB-u7YXVE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dce1cf1e-48e1-4e38-c4d6-47e8fd672eae"
      },
      "source": [
        "# We need to sync down the GitHub repo to run the benchmarks\n",
        "!git clone https://github.com/UofT-EcoSystem/hfta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'hfta'...\n",
            "remote: Enumerating objects: 1288, done.\u001b[K\n",
            "remote: Counting objects: 100% (318/318), done.\u001b[K\n",
            "remote: Compressing objects: 100% (141/141), done.\u001b[K\n",
            "remote: Total 1288 (delta 210), reused 232 (delta 170), pack-reused 970\u001b[K\n",
            "Receiving objects: 100% (1288/1288), 35.97 MiB | 32.03 MiB/s, done.\n",
            "Resolving deltas: 100% (787/787), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGpUu0yIYeFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de7ce5f1-b224-4a3d-a3d0-d67adb85355e"
      },
      "source": [
        "# Run the MobileNet V2 benchmark\n",
        "!python hfta/examples/mobilenet/main.py --version v2 --epochs 5 --amp --eval --dataset cifar10 --device cuda --lr 0.01 0.02 0.03 --hfta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-27 06:04:15.968667: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Enable cuDNN heuristics!\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/hfta/examples/mobilenet/../../datasets/cifar10/cifar-10-python.tar.gz\n",
            "170499072it [00:01, 89875879.80it/s]                   \n",
            "Extracting /content/hfta/examples/mobilenet/../../datasets/cifar10/cifar-10-python.tar.gz to /content/hfta/examples/mobilenet/../../datasets/cifar10\n",
            "Files already downloaded and verified\n",
            "Epoch 0 took 27.782680988311768 s!\n",
            "Epoch 1 took 15.300712585449219 s!\n",
            "Epoch 2 took 15.644184350967407 s!\n",
            "Epoch 3 took 15.658564567565918 s!\n",
            "Epoch 4 took 15.461482524871826 s!\n",
            "Running validation loop ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG1NV_nRifDA"
      },
      "source": [
        "Now, let's learn how to leverage HFTA on a normal PyTorch model in the following sections with a simple example of training a convolutional neural network on the MNIST dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6fX_L6cY55P"
      },
      "source": [
        "## Train a MNIST model without HFTA\n",
        "\n",
        "We train a simple neural network with two convolutional layers and two fully connected layers, together with some max pooling and dropout layers. This model is trained with the MNIST dataset to recognize hand-written images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9ZuRE7hZ5K_"
      },
      "source": [
        "### Define the model in the usual way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIipDP9YqC2v"
      },
      "source": [
        "import time\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "    self.max_pool2d = nn.MaxPool2d(2)\n",
        "    self.fc1 = nn.Linear(9216, 128)\n",
        "    self.fc2 = nn.Linear(128, 10)\n",
        "    self.dropout1 = nn.Dropout2d(0.25)\n",
        "    self.dropout2 = nn.Dropout2d(0.5)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.max_pool2d(x)\n",
        "    x = self.dropout1(x)\n",
        "    x = torch.flatten(x, -3)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.fc2(x)\n",
        "    output = F.log_softmax(x, dim=-1)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIlg5eVScoWS"
      },
      "source": [
        "### Define the training and testing loop for a single epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiJkw55Hcwp6"
      },
      "source": [
        "def train(config, model, device, train_loader, optimizer, epoch):\n",
        "  \"\"\"\n",
        "  config: a dict defined by users to control the experiment\n",
        "          See section: \"Train the model\"\n",
        "  model: class Net defined in the code block above\n",
        "  device: torch.device\n",
        "  train_loader: torch.utils.data.dataloader.DataLoader\n",
        "  optimizer: torch.optim\n",
        "  epoch: int\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % config[\"log_interval\"] == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch,\n",
        "          batch_idx * len(data),\n",
        "          len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader),\n",
        "          loss.item(),\n",
        "      ))\n",
        "      if config[\"dry_run\"]:\n",
        "        break\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "  \"\"\"\n",
        "  model: class Net defined in the code block above\n",
        "  device: torch.device\n",
        "  test_loader: torch.utils.data.dataloader.DataLoader\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      N = target.size(0)\n",
        "      output = model(data)\n",
        "      test_loss += F.nll_loss(output, target,\n",
        "                              reduction='none').view(-1, N).sum(dim=1)\n",
        "      pred = output.argmax(dim=1, keepdim=True)\n",
        "      correct += pred.eq(target.view_as(pred)).view(-1, N).sum(dim=1)\n",
        "\n",
        "  length = len(test_loader.dataset)\n",
        "  test_loss /= length\n",
        "  loss_str = [\"%.4f\" % e for e in test_loss]\n",
        "  correct_str = [\n",
        "      \"%d/%d(%.2lf%%)\" % (e, length, 100. * e / length) for e in correct\n",
        "  ]\n",
        "  print('Test set: \\tAverage loss: {}, \\n \\t\\t\\tAccuracy: {}\\n'.format(\n",
        "      loss_str, correct_str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43Nr19KJc4MT"
      },
      "source": [
        "### Define the main loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhts2SMCc7B4"
      },
      "source": [
        "def main(config):\n",
        "  \"\"\"\n",
        "  config: a dict defined by users to control the experiment\n",
        "  \"\"\"\n",
        "  random.seed(1)\n",
        "  np.random.seed(1)\n",
        "  torch.manual_seed(1)\n",
        "\n",
        "  device = torch.device(config[\"device\"])\n",
        "\n",
        "  kwargs = {'batch_size': config[\"batch_size\"]}\n",
        "  kwargs.update({'num_workers': 1, 'pin_memory': True, 'shuffle': True},)\n",
        "\n",
        "  transform = transforms.Compose(\n",
        "      [transforms.ToTensor(),\n",
        "       transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "  dataset1 = datasets.MNIST('./data',\n",
        "                            train=True,\n",
        "                            download=True,\n",
        "                            transform=transform)\n",
        "  dataset2 = datasets.MNIST('./data', train=False, transform=transform)\n",
        "  train_loader = torch.utils.data.DataLoader(dataset1, **kwargs)\n",
        "  test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)\n",
        "\n",
        "  model = Net().to(device)\n",
        "\n",
        "  optimizer = optim.Adadelta(\n",
        "      model.parameters(),\n",
        "      lr=config[\"lr\"][0],\n",
        "  )\n",
        "\n",
        "  start = time.perf_counter()\n",
        "  for epoch in range(1, config[\"epochs\"] + 1):\n",
        "    now = time.perf_counter()\n",
        "    train(config, model, device, train_loader, optimizer, epoch)\n",
        "    print('Epoch {} took {} s!'.format(epoch, time.perf_counter() - now))\n",
        "  end = time.perf_counter()\n",
        "\n",
        "  test(model, device, test_loader)\n",
        "\n",
        "  print('All jobs Finished, Each epoch took {} s on average!'.format(\n",
        "      (end - start) / config[\"epochs\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35jv_fzP-llU"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VwstttHmxZF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "51675c8444e8435ca60180706ab7cd8e",
            "35a868d5339e48b29ce6e643c3e75f15",
            "5d8a5c92f97e40468a0f8379c98bfaa3",
            "c6bcd6e7256649929273f9061ff48f21",
            "84634eff4a1342359cd0cfdeeb2b6107",
            "77a998ae706448ba8d54c4ae5b55cd3d",
            "1ede96c4d4ca479e85076ebc6dc80889",
            "2423ed4ce4b54d56ab4bc501bd02e3ea",
            "32f0a9145f404d8d9e58c76f1bcf7fc3",
            "8fa0d5f8af9542238c744583af69f615",
            "28c5a87170014d15b1547ccb8eb53750",
            "8448978b77674b04ba52ea3bae1c0081",
            "d40617c96b194e4ba1f88c564eb14c0b",
            "526ccac9a4f04b5db6883109da8fd1bb",
            "271a71dc2370468c994f439635af4bc5",
            "a2b9c7ec480549b1879e49ee4bf17da6",
            "28713bf12a84482fa48f34657309b970",
            "3d15e9e7b5ce45e7b64ed44a7c512338",
            "02b386588cfb425bb8b8d1da37ca23ae",
            "6f82ca214b4a4f2a86d055895a2d3737",
            "442f750c5d164eca9b876197d8c4d81b",
            "09efc251cf604e8886d3c2dc67e68a07",
            "f5fa8090913f4ff5aef2eca6756d02e2",
            "93cfebb4ebf74b0286e006f1c2c495f3",
            "d7810aa95fc14a74831de82d54c8bbd7",
            "d67de91d55e041f1a7b5bd1a91a08793",
            "af849cb35c9146948ad72a1962881394",
            "7c608a5ce48d4602acf4567d4092caed",
            "1642a6479022482db6b5ed3c66da8864",
            "a9e5522d0c9a4a518afbc26740348030",
            "78b27e47984241c0a3b6f6afd2b42d94",
            "cd3f95d06f404c1eadf5f33cbe187023"
          ]
        },
        "outputId": "1b90dc7b-8a3e-476b-a4b4-b327c9d4ec98"
      },
      "source": [
        "config = {\n",
        "    \"device\": \"cuda\",  # choose from cuda and cpu\n",
        "    \"batch_size\": 64,\n",
        "    \"lr\": [1.0],\n",
        "    \"gamma\": 0.7,\n",
        "    \"epochs\": 4,\n",
        "    \"seed\": 1,\n",
        "    \"log_interval\": 500,\n",
        "    \"dry_run\": False,\n",
        "    \"save_model\": False,\n",
        "}\n",
        "\n",
        "print(config)\n",
        "main(config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'device': 'cuda', 'batch_size': 64, 'lr': [1.0], 'gamma': 0.7, 'epochs': 4, 'seed': 1, 'log_interval': 500, 'dry_run': False, 'save_model': False}\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51675c8444e8435ca60180706ab7cd8e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32f0a9145f404d8d9e58c76f1bcf7fc3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28713bf12a84482fa48f34657309b970",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7810aa95fc14a74831de82d54c8bbd7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.320749\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.066510\n",
            "Epoch 1 took 11.140597851000024 s!\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.085425\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.029257\n",
            "Epoch 2 took 11.133023817000009 s!\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.036951\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.170459\n",
            "Epoch 3 took 11.134598162999993 s!\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.029309\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.040325\n",
            "Epoch 4 took 11.170153673999977 s!\n",
            "Test set: \tAverage loss: ['0.0328'], \n",
            " \t\t\tAccuracy: ['9896/10000(98.96%)']\n",
            "\n",
            "All jobs Finished, Each epoch took 11.144698025499999 s on average!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzr_a67BZyJc"
      },
      "source": [
        "## Improve hardware utilization with HFTA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcehAMUiaE3Q"
      },
      "source": [
        "### How to modify a mnist model to use HFTA?\n",
        "\n",
        "Our convolutional neural network was training fine with MNIST, and that's great! However, with such a small model and a small batch size, the underlying accelerator (NVIDIA GPU in this case) is likely going to be under-utilized. Thus, how can we possibly increase the hardware utilization for this training workload?\n",
        "\n",
        "If this training workload is used under a repetitive setting (e.g., hyper-parameter tuning or convergence stability testing), hardware utilization can be directly increased by horizontally fusing multiple training workloads together, such that multiple models are trained on the same accelerator (e.g., GPU) at the same time.\n",
        "\n",
        "However, fusing training workloads manually could be cumbersome and error-prone. Thus, the HFTA library provides convenient utilities to facilitate the effort of horizontally fusing models. Now, let us take a look into how we can easily perform the horizontal model fusion.\n",
        "\n",
        "Please check the comments in the code to understand what needs to be done. In this example, we fuse multiple models (where the number of models is controlled by the parameter `B`) with different learning rates together via HFTA to improve the hardware utilization.\n",
        "\n",
        "Be aware that this is just a very simple example of tuning the learning rate. However, in general, many other use cases might be applicable. For example: testing the convergence with different seeds; trying different weight initializers; or even ensemble learning.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqz5n9L3qLcw"
      },
      "source": [
        "#### Modify the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQGHQtpReYWA"
      },
      "source": [
        "from __future__ import print_function\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Use utilities from the hfta package to convert your operators and optimizors.\n",
        "from hfta.ops import convert_ops\n",
        "from hfta.optim import get_hfta_optim_for\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "  # When initializing the model, save the number of models that need to be fused \n",
        "  # (B), and convert the default operators to their HFTA version with \n",
        "  # convert_ops(B, list of operators).\n",
        "  # When passing 0 to B, we train the model as it is without enabling HFTA.\n",
        "  def __init__(self, B=0):\n",
        "    super(Net, self).__init__()\n",
        "    # Convert default operators to their HFTA version.\n",
        "    (Conv2d, MaxPool2d, Linear, Dropout2d) = convert_ops(\n",
        "        B,\n",
        "        nn.Conv2d,\n",
        "        nn.MaxPool2d,\n",
        "        nn.Linear,\n",
        "        nn.Dropout2d,\n",
        "    )\n",
        "\n",
        "    # Define the model with converted operators as if they were unchanged.\n",
        "    self.B = B\n",
        "    self.conv1 = Conv2d(1, 32, 3, 1)\n",
        "    self.conv2 = Conv2d(32, 64, 3, 1)\n",
        "    self.max_pool2d = MaxPool2d(2)\n",
        "    self.fc1 = Linear(9216, 128)\n",
        "    self.fc2 = Linear(128, 10)\n",
        "    self.dropout1 = Dropout2d(0.25)\n",
        "    self.dropout2 = Dropout2d(0.5)\n",
        "\n",
        "  # Minor modifications to the forward pass on special operators.\n",
        "  # Check the documentation of each operator for details.\n",
        "  # Now the shape of x is [batch size, B, 3, 28, 28].\n",
        "  # This means that the input images to all B models are concatenated along the \n",
        "  # channel dimension.\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.max_pool2d(x)\n",
        "    x = self.dropout1(x)\n",
        "\n",
        "    x = torch.flatten(x, -3)\n",
        "    if self.B > 0:\n",
        "      # The output shape from flatten is [batch size, B, features], where\n",
        "      # features == channels * height * width from dropout1.\n",
        "      # However, fc1 expects the input shape to be [B, batch size, features].\n",
        "      # Thus, we need to transpose the first and second dimensions.\n",
        "      x = x.transpose(0, 1)\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.fc2(x)\n",
        "    output = F.log_softmax(x, dim=-1)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6Bn9_jCqQTv"
      },
      "source": [
        "#### Modify the training and testing loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjokanLrqTyp"
      },
      "source": [
        "def train(config, model, device, train_loader, optimizer, epoch, B):\n",
        "  \"\"\"\n",
        "  config: a dict defined by users to control the experiment\n",
        "          See section: \"Train the model\"\n",
        "  model: class Net defined in the code block above\n",
        "  device: torch.device\n",
        "  train_loader: torch.utils.data.dataloader.DataLoader\n",
        "  optimizer: torch.optim\n",
        "  epoch: int\n",
        "  B: int, the number of models to be fused. When B == 0, we train the original \n",
        "     model as it is without enabling HFTA.\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Need to duplicate a single batch of input images into multiple batches to \n",
        "    # feed into the fused model.\n",
        "    if B > 0:\n",
        "      N = target.size(0)\n",
        "      data = data.unsqueeze(1).expand(-1, B, -1, -1, -1)\n",
        "      target = target.repeat(B)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "\n",
        "    # Also need to modify the loss function to take consideration on the fused \n",
        "    # model.\n",
        "    # In the case:\n",
        "    #   1) the loss function is reduced by averaging along the batch dimension.\n",
        "    #   2) multiple models are horizontally fused via HFTA.\n",
        "    # To make sure the mathematically equivalent gradients are derived by \n",
        "    # \".backward()\", we need to scale the loss value by B.\n",
        "    # You might refer to our paper for why such scaling is needed.\n",
        "    if B > 0:\n",
        "      loss = B * F.nll_loss(output.view(B * N, -1), target)\n",
        "    else:\n",
        "      loss = F.nll_loss(output, target)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % config[\"log_interval\"] == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "      if config[\"dry_run\"]:\n",
        "        break\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, B):\n",
        "  \"\"\"\n",
        "  model: class Net defined in the code block above\n",
        "  device: torch.device\n",
        "  test_loader: torch.utils.data.dataloader.DataLoader\n",
        "  B: int, the number of models to be fused. When B == 0, we test the original \n",
        "     model as it is without enabling HFTA.\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      N = target.size(0)\n",
        "\n",
        "      # Need to duplicate a single batch of input images into multiple batches \n",
        "      # to feed into the fused model.\n",
        "      if B > 0:\n",
        "        data = data.unsqueeze(1).expand(-1, B, -1, -1, -1)\n",
        "        target = target.repeat(B)\n",
        "\n",
        "      output = model(data)\n",
        "\n",
        "      # Change the shape of the output to align with the loss function.\n",
        "      if B > 0:\n",
        "        output = output.view(B * N, -1)\n",
        "\n",
        "      test_loss += F.nll_loss(output, target,\n",
        "                              reduction='none').view(-1, N).sum(dim=1)\n",
        "      pred = output.argmax(dim=1, keepdim=True)\n",
        "      correct += pred.eq(target.view_as(pred)).view(-1, N).sum(dim=1)\n",
        "\n",
        "  length = len(test_loader.dataset)\n",
        "  test_loss /= length\n",
        "  loss_str = [\"%.4f\" % e for e in test_loss]\n",
        "  correct_str = [\n",
        "      \"%d/%d(%.2lf%%)\" % (e, length, 100. * e / length) for e in correct\n",
        "  ]\n",
        "  print('Test set: \\tAverage loss: {}, \\n \\t\\t\\tAccuracy: {}\\n'.format(\n",
        "      loss_str, correct_str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0Fvlq5Sqc2v"
      },
      "source": [
        "#### Modify the main loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00ENhJCRqfAU"
      },
      "source": [
        "def main(config):\n",
        "  \"\"\"\n",
        "  config: a dict defined by users to control the experiment\n",
        "  \"\"\"\n",
        "  random.seed(config[\"seed\"])\n",
        "  np.random.seed(config[\"seed\"])\n",
        "  torch.manual_seed(config[\"seed\"])\n",
        "\n",
        "  device = torch.device(config[\"device\"])\n",
        "\n",
        "  kwargs = {'batch_size': config[\"batch_size\"]}\n",
        "  kwargs.update({'num_workers': 1, 'pin_memory': True, 'shuffle': True},)\n",
        "\n",
        "  transform = transforms.Compose(\n",
        "      [transforms.ToTensor(),\n",
        "       transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "  # Determine the number of models that are horizontally fused together from the \n",
        "  # number of provided learning rates that need to be tested.\n",
        "  B = len(config[\"lr\"]) if config[\"use_hfta\"] else 0\n",
        "\n",
        "  dataset1 = datasets.MNIST('./data',\n",
        "                            train=True,\n",
        "                            download=True,\n",
        "                            transform=transform)\n",
        "  dataset2 = datasets.MNIST('./data', train=False, transform=transform)\n",
        "  train_loader = torch.utils.data.DataLoader(dataset1, **kwargs)\n",
        "  test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)\n",
        "\n",
        "  # Specify the number of models that need to be fused horizontally together (B)\n",
        "  # and create the fused model.\n",
        "  model = Net(B).to(device)\n",
        "\n",
        "  print('B={} lr={}'.format(B, config[\"lr\"]), file=sys.stderr)\n",
        "\n",
        "  # Convert the default optimizor (PyTorch Adadelta) to its HFTA version with \n",
        "  # get_hfta_optim_for(<default>, B).\n",
        "  optimizer = get_hfta_optim_for(optim.Adadelta, B=B)(\n",
        "      model.parameters(),\n",
        "      lr=config[\"lr\"] if B > 0 else config[\"lr\"][0],\n",
        "  )\n",
        "\n",
        "  start = time.perf_counter()\n",
        "  for epoch in range(1, config[\"epochs\"] + 1):\n",
        "    now = time.perf_counter()\n",
        "    train(config, model, device, train_loader, optimizer, epoch, B)\n",
        "    print('Epoch {} took {} s!'.format(epoch, time.perf_counter() - now))\n",
        "  end = time.perf_counter()\n",
        "\n",
        "  test(model, device, test_loader, B)\n",
        "\n",
        "  print('All jobs Finished, Each epoch took {} s on average!'.format(\n",
        "      (end - start) / (max(B, 1) * config[\"epochs\"])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EKBmbnYaM63"
      },
      "source": [
        "### Train a single HFTA-enabled model with MNIST\n",
        "\n",
        "Note that this run may be slightly slower than the non-HFTA version because enabling HFTA might lead to a small amount of overhead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t6_2bCoabCN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2094c706-428b-4821-e527-dd1f8ed80725"
      },
      "source": [
        "# Enable HFTA to train only a single model.\n",
        "# Only 1 model is trained\n",
        "config = {\n",
        "    \"use_hfta\": True,\n",
        "    \"device\": \"cuda\",  # choose from cuda and cpu\n",
        "    \"batch_size\": 64,\n",
        "    \"lr\": [0.1],\n",
        "    \"gamma\": 0.7,\n",
        "    \"epochs\": 4,\n",
        "    \"seed\": 1,\n",
        "    \"log_interval\": 500,\n",
        "    \"dry_run\": False,\n",
        "    \"save_model\": False,\n",
        "}\n",
        "\n",
        "print(config)\n",
        "main(config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'use_hfta': True, 'device': 'cuda', 'batch_size': 64, 'lr': [0.1], 'gamma': 0.7, 'epochs': 4, 'seed': 1, 'log_interval': 500, 'dry_run': False, 'save_model': False}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "B=1 lr=[0.1]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.295558\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.077447\n",
            "Epoch 1 took 11.392657968999998 s!\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.136890\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.045705\n",
            "Epoch 2 took 11.238287230000026 s!\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.111560\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.058608\n",
            "Epoch 3 took 11.479781749999972 s!\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.052281\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.056059\n",
            "Epoch 4 took 11.362106541000003 s!\n",
            "Test set: \tAverage loss: ['0.0392'], \n",
            " \t\t\tAccuracy: ['9868/10000(98.68%)']\n",
            "\n",
            "All jobs Finished, Each epoch took 11.368499729249997 s on average!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6JFJ8laac_b"
      },
      "source": [
        "### Enable HFTA to train multiple models in the fused form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N44NF4HoalOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2767938b-3ea5-4d68-9cb8-028d6719c2c6"
      },
      "source": [
        "# Enable HFTA and fuse 6 models\n",
        "config = {\n",
        "    \"use_hfta\": True,\n",
        "    \"device\": \"cuda\",  # choose from cuda and cpu\n",
        "    \"batch_size\": 64,\n",
        "    \"lr\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
        "    \"gamma\": 0.7,\n",
        "    \"epochs\": 4,\n",
        "    \"seed\": 1,\n",
        "    \"log_interval\": 500,\n",
        "    \"dry_run\": False,\n",
        "    \"save_model\": False,\n",
        "}\n",
        "\n",
        "print(config)\n",
        "main(config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'use_hfta': True, 'device': 'cuda', 'batch_size': 64, 'lr': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6], 'gamma': 0.7, 'epochs': 4, 'seed': 1, 'log_interval': 500, 'dry_run': False, 'save_model': False}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "B=6 lr=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 13.828041\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.799471\n",
            "Epoch 1 took 20.78268953999998 s!\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.668073\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.625135\n",
            "Epoch 2 took 20.50058612500004 s!\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.187476\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.931624\n",
            "Epoch 3 took 20.27082759299998 s!\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.565796\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.624585\n",
            "Epoch 4 took 20.285229027000014 s!\n",
            "Test set: \tAverage loss: ['0.0435', '0.0322', '0.0327', '0.0361', '0.0312', '0.0354'], \n",
            " \t\t\tAccuracy: ['9854/10000(98.54%)', '9894/10000(98.94%)', '9883/10000(98.83%)', '9879/10000(98.79%)', '9892/10000(98.92%)', '9887/10000(98.87%)']\n",
            "\n",
            "All jobs Finished, Each epoch took 3.410018879708334 s on average!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV7X0OgbBfxW"
      },
      "source": [
        "From the [Train the model](https://colab.research.google.com/drive/1gSW6PpWAKfHI3GCxOmSrbBS5PFzh7HEl#scrollTo=35jv_fzP-llU) section above, we would know that, if we want to test 6 different learning rates and train each model on a separate GPU, we would need `11.14 * 6 = 66.84` GPU seconds per epoch on average. As we can see, with HFTA, we can reduce the average training time for testing 6 different learning rates to `3.41 * 6 = 20.46` GPU seconds per epoch, thus, improving the overall utilization of the GPU and reducing the overall training time by `66.84 / 20.46 = 3.27x`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFEmZ5zq-llk"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Based on the time each epoch takes when training the non-HFTA and HFTA version of the same model, we can see that HFTA helps to increase the throughput of the training, especially on a large hardware. Check our [paper](https://arxiv.org/pdf/2102.02344.pdf) for more details."
      ]
    }
  ]
}